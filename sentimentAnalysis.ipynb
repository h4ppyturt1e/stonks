{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\",\n",
    "             \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "\n",
    "\n",
    "def readCSV(filename):\n",
    "    \"\"\"Reads a CSV file and returns a pandas DataFrame\"\"\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"training_data/finDataset.csv\"\n",
    "finDf = readCSV(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>RISING costs have forced packaging producer Hu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>Nordic Walking was first used as a summer trai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5839</th>\n",
       "      <td>According shipping company Viking Line , the E...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>In the building and home improvement trade , s...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>HELSINKI AFX - KCI Konecranes said it has won ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5842 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence Sentiment\n",
       "0     The GeoSolutions technology will leverage Bene...  positive\n",
       "1     $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2     For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3     According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4     The Swedish buyout firm has sold its remaining...   neutral\n",
       "...                                                 ...       ...\n",
       "5837  RISING costs have forced packaging producer Hu...  negative\n",
       "5838  Nordic Walking was first used as a summer trai...   neutral\n",
       "5839  According shipping company Viking Line , the E...   neutral\n",
       "5840  In the building and home improvement trade , s...   neutral\n",
       "5841  HELSINKI AFX - KCI Konecranes said it has won ...  positive\n",
       "\n",
       "[5842 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "Before feeding the data into a neural network, it often needs to be preprocessed:\n",
    "\n",
    "- Clean the Text: Remove unnecessary characters, convert to lowercase, and possibly tokenize the sentences.\n",
    "- Split the Data: Divide your data into training and test sets.\n",
    "- Encode Labels: If your sentiment labels are textual (like 'positive', 'negative'), encode them into numerical format.\n",
    "- Vectorize Data: Transform each text into a sequence of integers corresponding to the indices of individual tokens in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(sentence):\n",
    "    \"\"\"Removes stopwords from a sentence\"\"\"\n",
    "    splitSentence = sentence.lower().split()\n",
    "    splitSentence = [word for word in splitSentence if word not in STOPWORDS]\n",
    "    processedSentence = \" \".join(splitSentence)\n",
    "\n",
    "    return processedSentence\n",
    "\n",
    "def cleanData(df):\n",
    "    \"\"\"Cleans the data by removing stopwords and NaN values\"\"\"\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df[\"Sentence\"] = df[\"Sentence\"].apply(removeStopwords)\n",
    "\n",
    "    return df\n",
    "\n",
    "def splitData(df):\n",
    "    \"\"\"Splits the data into training and testing data\"\"\"\n",
    "\n",
    "    trainData, testData, trainLabels, testLabels = train_test_split(df[\"Sentence\"], df[\"Sentiment\"], test_size=0.2, stratify=df[\"Sentiment\"])\n",
    "    \n",
    "    return trainData, testData, trainLabels, testLabels\n",
    "\n",
    "def vectorizeData(trainData, testData):\n",
    "    \"\"\"Tokenizes the data\"\"\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    trainData_vectorized = vectorizer.fit_transform(trainData)\n",
    "    testData_vectorized = vectorizer.transform(testData)\n",
    "    \n",
    "    return trainData_vectorized, testData_vectorized, len(vectorizer.get_feature_names_out())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDf = cleanData(finDf)\n",
    "trainData, testData, trainLabels, testLabels = splitData(finDf)\n",
    "trainData_vectorized, testData_vectorized, INPUT_DIM = vectorizeData(trainData, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network Model\n",
    "For sentiment analysis, a common approach is to use a recurrent neural network (RNN) with Long Short-Term Memory (LSTM) or a Convolutional Neural Network (CNN). Frameworks like TensorFlow or PyTorch can be used for this.\n",
    "\n",
    "## Compile the Model\n",
    "Choose an optimizer (like Adam), a loss function (like binary crossentropy for binary classification), and metrics (like accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_dim, output_dim, max_sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=max_sequence_length))\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # (positive, neutral, negative) - 3 neurons\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_cnn_model(input_dim, output_dim, max_sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=max_sequence_length))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(units=10, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # (positive, neutral, negative) - 3 neurons\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 10160, 128)        1300480   \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 10160, 50)         35800     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 10160, 50)         0         \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1356633 (5.18 MB)\n",
      "Trainable params: 1356633 (5.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create LSTM model\n",
    "model_lstm = create_lstm_model(input_dim=trainData_vectorized.shape[1], output_dim=128, max_sequence_length=trainData_vectorized.shape[1])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 10160, 128)        1300480   \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 10156, 128)        82048     \n",
      "                                                                 \n",
      " global_max_pooling1d_4 (Gl  (None, 128)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1383851 (5.28 MB)\n",
      "Trainable params: 1383851 (5.28 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create CNN model\n",
    "model_cnn = create_cnn_model(input_dim=trainData_vectorized.shape[1], output_dim=128, max_sequence_length=trainData_vectorized.shape[1])\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Feed your training data into the model. This process involves tuning hyperparameters like batch size and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData_vectorized = trainData_vectorized.toarray()\n",
    "testData_vectorized = testData_vectorized.toarray()\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "trainLabels_encoded = encoder.fit_transform(trainLabels)\n",
    "testLabels_encoded = encoder.transform(testLabels)\n",
    "\n",
    "# one-hot encode labels\n",
    "trainLabels_oneHot = to_categorical(trainLabels_encoded)\n",
    "testLabels_oneHot = to_categorical(testLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 19/117 [===>..........................] - ETA: 15:37 - loss: 1.0233 - accuracy: 0.5115"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# train LSTM model\n",
    "lstm_model = create_lstm_model(input_dim=INPUT_DIM, output_dim=128, max_sequence_length=trainData_vectorized.shape[1])\n",
    "\n",
    "history_lstm = lstm_model.fit(trainData_vectorized, trainLabels_oneHot, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# train CNN model\n",
    "cnn_model = create_cnn_model(input_dim=INPUT_DIM, output_dim=128, max_sequence_length=trainData_vectorized.shape[1])\n",
    "\n",
    "history_cnn = cnn_model.fit(trainData_vectorized, trainLabels_oneHot, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Test the model on your test set to see how well it generalizes to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate LSTM model\n",
    "test_loss, test_accuracy = lstm_model.evaluate(testData_vectorized, testLabels_oneHot)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# plot\n",
    "plt.plot(history_lstm.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate CNN model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(testData_vectorized, testLabels_oneHot)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# plot\n",
    "plt.plot(history_cnn.history['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
